{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462a319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words extracted: 178124\n"
     ]
    }
   ],
   "source": [
    "#Extracting words from one PDF\n",
    "#Extracted words are combined into a single string\n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "\n",
    "pdf_path = r\"C:\\Users\\makaa\\Documents\\Comp Sci\\RAG PROJECT\\CUDA_C_Programming_Guide.pdf\"\n",
    "\n",
    "# Extract text from the PDF\n",
    "text = extract_text(pdf_path)\n",
    "\n",
    "# Optional: Clean up the text by replacing newline characters\n",
    "clean_text = text.replace('\\n', ' ').strip()\n",
    "\n",
    "# Use a regular expression to extract words\n",
    "# This regex matches sequences of alphanumeric characters (words)\n",
    "words = re.findall(r'\\b\\w+\\b', clean_text)\n",
    "\n",
    "# Output the results\n",
    "print(\"Total words extracted:\", len(words))\n",
    "\n",
    "# Write the extracted words to a file (joined by spaces)\n",
    "with open(\"extracted_words.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\" \".join(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c16e9228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\makaa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 595 chunks.\n",
      "First chunk:\n",
      "CUDA C Programming Guide Release 12 8 NVIDIA Corporation Feb 14 2025 Contents 1 The Benefits of Using GPUs 2 CUDA A General Purpose Parallel Computing Platform and Programming Model 3 A Scalable Programming Model 4 Document Structure 5 Programming Model 5 1 5 2 5 2 1 5 3 5 4 5 5 5 5 1 5 6 Kernels Thread Hierarchy Thread Block Clusters Memory Hierarchy Heterogeneous Programming Asynchronous SIMT Programming Model Asynchronous Operations Compute Capability 6 Programming Interface CUDA Runtime Compilation with NVCC 6 1 1 1 6 1 1 2 6 1 6 1 1 6 1 2 6 1 3 6 1 4 6 1 5 6 1 6 6 2 6 2 1 6 2 2 6 2 3 6 2 3 1 6 2 3 2 6 2 3 3 6 2 3 4 6 2 3 5 6 2 3 6 6 2 3 7 6 2 3 8 6 2 4 6 2 5 6 2 6 6 2 6 1 Compilation Workflow Binary Compatibility PTX Compatibility Application Compatibility C Compatibility 64 Bit Compatibility Offline Compilation Just in Time Compilation Initialization Device Memory Device Memory L2 Access Management L2 Cache Set Aside for Persisting Accesses L2 Policy for Persisting Accesses L2 Access Properties L2 Persistence Example Reset L2 Access to Normal Manage Utilization of L2 set aside cache Query L2 cache Properties Control L2 Cache Set Aside Size for Persisting Memory Access Shared Memory Distributed Shared Memory Page Locked Host Memory Portable Memory 3 5 7 9 11 11 12 14 16 17 17 17 19 21 21 22 22 22 22 23 23 24 24 24 25 26 29 29 30 31 31 32 33 33 33 34 40 42 43 i 6 2 8 6 2 7 1 6 2\n"
     ]
    }
   ],
   "source": [
    "#Prepare Text for Chunking\n",
    "# Read the cleaned text file (if not already in memory)\n",
    "with open(\"extracted_words.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    clean_text = file.read()\n",
    "    \n",
    "#Using NLTKs tokenizer I am going to split the text into words and then grup them into chunks of 300 tokens.\n",
    "import nltk\n",
    "nltk.download('punkt')  # Run this once to download the tokenizer models\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(clean_text)\n",
    "\n",
    "# Set chunk size (number of words per chunk)\n",
    "chunk_size = 300\n",
    "\n",
    "# Create chunks: group every 300 tokens into a single chunk\n",
    "chunks = [\" \".join(tokens[i:i + chunk_size]) for i in range(0, len(tokens), chunk_size)]\n",
    "\n",
    "print(f\"Generated {len(chunks)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5fbd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'knowledge_base.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "#Saving chunks into a CSV file, with columns for an identifier, the text chunk, and a source label (indicating these come from CUDA documentation).\n",
    "#Create a DataFrame for the knowledge base\n",
    "data = {\n",
    "    \"id\": list(range(1, len(chunks) + 1)),\n",
    "    \"text\": chunks,\n",
    "    \"source\": [\"CUDA Documentation\"] * len(chunks)  # You can update this if you add more sources later\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"knowledge_base.csv\", index=False)\n",
    "print(\"CSV file 'knowledge_base.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec3aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
