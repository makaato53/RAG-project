{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bbc4f9-7229-4190-af73-56cdef25c7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makaa\\anaconda3\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import openai\n",
    "import pickle\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48b434d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 595 chunks from the knowledge base\n",
      "Creating embeddings for the knowledge base...\n",
      "Loading the embedding model...\n",
      "Creating embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e84619b20848599fdb6f77c9ff1b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created embeddings with shape: (595, 384)\n",
      "Saving embeddings...\n"
     ]
    }
   ],
   "source": [
    "#Loading existing knowledge base\n",
    "df = pd.read_csv('knowledge_base.csv')\n",
    "print(f\"Loaded {len(df)} chunks from the knowledge base\")\n",
    "\n",
    "# Create embeddings\n",
    "def create_embeddings(texts: List[str]) -> np.ndarray:\n",
    "    \"\"\"Create embeddings for a list of texts using Sentence Transformers.\"\"\"\n",
    "    print(\"Loading the embedding model...\")\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    print(\"Creating embeddings...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"Created embeddings with shape: {embeddings.shape}\")\n",
    "    return embeddings, model\n",
    "\n",
    "# Create embeddings for our knowledge base\n",
    "print(\"Creating embeddings for the knowledge base...\")\n",
    "embeddings, model = create_embeddings(df['text'].tolist())\n",
    "# Save embeddings and dataframe\n",
    "print(\"Saving embeddings...\")\n",
    "with open('cuda_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump((df, embeddings), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd0343e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up FAISS index...\n",
      "\n",
      "Testing search functionality:\n",
      "\n",
      "Query: What is CUDA and how does it work?\n",
      "\n",
      "Relevant passages:\n",
      "\n",
      "1. Text: C Programming Guide Release 12 8 498 Chapter 19 Compute Capabilities Chapter 20 Driver API This section assumes knowledge of the concepts described in CUDA Runtime The driver API is implemented in the...\n",
      "Source: CUDA Documentation\n",
      "Similarity Score: 0.642\n",
      "\n",
      "2. Text: introduces the low level driver API CUDA Environment Variables lists all the CUDA environment variables Unified Memory Programming introduces the Unified Memory programming model 9 CUDA C Programming ...\n",
      "Source: CUDA Documentation\n",
      "Similarity Score: 0.637\n",
      "\n",
      "3. Text: cudaSetDevice will now explicitly initialize the runtime after changing the current device for the host thread Previous versions of CUDA delayed runtime initialization on the new device until the firs...\n",
      "Source: CUDA Documentation\n",
      "Similarity Score: 0.588\n",
      "\n",
      "Query: Explain CUDA thread hierarchy\n",
      "\n",
      "Relevant passages:\n",
      "\n",
      "1. Text: Child Grids have also completed Child A Child thread block or grid is one that has been launched by a Parent grid A Child grid must complete before the Parent Thread Thread Block or Grid are considere...\n",
      "Source: CUDA Documentation\n",
      "Similarity Score: 0.726\n",
      "\n",
      "2. Text: introduces the low level driver API CUDA Environment Variables lists all the CUDA environment variables Unified Memory Programming introduces the Unified Memory programming model 9 CUDA C Programming ...\n",
      "Source: CUDA Documentation\n",
      "Similarity Score: 0.707\n",
      "\n",
      "3. Text: cg cooperative_groups template cuda thread_scope Scope __device__ unsigned int atomicAddOneRelaxed cuda atomic unsigned int Scope atomic auto g cg coalesced_threads auto prev cg invoke_one_broadcast g...\n",
      "Source: CUDA Documentation\n",
      "Similarity Score: 0.685\n",
      "\n",
      "Query: How does CUDA memory management work?\n",
      "\n",
      "Relevant passages:\n",
      "\n",
      "1. Text: the next iteration if some other rank is slow cuda atomic_ref unsigned int cuda thread_scope_system ac arrival_counter_uc while expected_count ac load cuda memory_order_acquire Atomic load reduction f...\n",
      "Source: CUDA Documentation\n",
      "Similarity Score: 0.737\n",
      "\n",
      "2. Text: among others Linux HMM requires Linux kernel version 6 1 24 6 2 11 or 6 3 devices with compute capability 7 5 or higher and a CUDA driver version 535 installed with Open Kernel Modules concurrentManag...\n",
      "Source: CUDA Documentation\n",
      "Similarity Score: 0.724\n",
      "\n",
      "3. Text: s strncpy s Hello Unified Memory n 99 Here we pass s to a kernel without explicitly copying printme 1 1 s cudaDeviceSynchronize Free as for normal CUDA allocations cudaFree s return 0 22 1 Unified Mem...\n",
      "Source: CUDA Documentation\n",
      "Similarity Score: 0.720\n"
     ]
    }
   ],
   "source": [
    "def setup_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n",
    "    \"\"\"Create and populate a FAISS index for similarity search.\"\"\"\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(np.array(embeddings).astype('float32'))\n",
    "    return index\n",
    "\n",
    "def search_knowledge_base(query: str, \n",
    "                         model: SentenceTransformer,\n",
    "                         index: faiss.IndexFlatL2,\n",
    "                         df: pd.DataFrame,\n",
    "                         top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Search for relevant CUDA documentation based on the query.\"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = model.encode([query])[0]\n",
    "    \n",
    "    # Search in FAISS\n",
    "    distances, indices = index.search(np.array([query_embedding]).astype('float32'), top_k)\n",
    "    \n",
    "    # Get the text of the most similar chunks\n",
    "    results = [\n",
    "        {\n",
    "            'text': df.iloc[idx]['text'],\n",
    "            'source': df.iloc[idx]['source'],\n",
    "            'similarity': 1 - dist/2  # Convert L2 distance to similarity score\n",
    "        }\n",
    "        for dist, idx in zip(distances[0], indices[0])\n",
    "    ]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create FAISS index\n",
    "print(\"Setting up FAISS index...\")\n",
    "index = setup_faiss_index(embeddings)\n",
    "\n",
    "# Let's test the search with some CUDA-related queries\n",
    "test_queries = [\n",
    "    \"What is CUDA and how does it work?\",\n",
    "    \"Explain CUDA thread hierarchy\",\n",
    "    \"How does CUDA memory management work?\"\n",
    "]\n",
    "\n",
    "def test_search_functionality(queries: List[str]):\n",
    "   \"\"\"\n",
    "   Test search functionality with multiple queries and display results.\n",
    "   \n",
    "   Args:\n",
    "       queries (List[str]): List of queries to test\n",
    "   \"\"\"\n",
    "   print(\"\\nTesting search functionality:\")\n",
    "   for query in queries:\n",
    "       print(f\"\\nQuery: {query}\")\n",
    "       results = search_knowledge_base(query, model, index, df)\n",
    "       print(\"\\nRelevant passages:\")\n",
    "       for i, result in enumerate(results, 1):\n",
    "           print(f\"\\n{i}. Text: {result['text'][:200]}...\")  # Show first 200 characters\n",
    "           print(f\"Source: {result['source']}\")\n",
    "           print(f\"Similarity Score: {result['similarity']:.3f}\")\n",
    "\n",
    "# Define test queries\n",
    "test_queries = [\n",
    "   \"What is CUDA and how does it work?\",\n",
    "   \"Explain CUDA thread hierarchy\",\n",
    "   \"How does CUDA memory management work?\"\n",
    "]\n",
    "test_search_functionality(test_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f86b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "def generate_response_hf(query: str, relevant_chunks: List[Dict], model_name=\"google/flan-t5-small\"):\n",
    "    \"\"\"\n",
    "    Generate response using Hugging Face T5 model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        \n",
    "        # Prepare context and prompt\n",
    "        context = \"\\n\\n\".join([chunk['text'] for chunk in relevant_chunks])\n",
    "        prompt = f\"\"\"Use the following CUDA documentation to answer the question.\n",
    "        \n",
    "        Documentation:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        # Tokenize and generate\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = model.generate(**inputs, max_length=200, num_beams=4, temperature=0.7)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\"\n",
    "# Now let's create a complete function that combines search and response generation\n",
    "def cuda_rag_response(query: str, top_k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Search relevant chunks and generate a response.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question\n",
    "        top_k (int): Number of relevant chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Contains the generated answer and the relevant chunks used\n",
    "    \"\"\"\n",
    "    # Get relevant chunks\n",
    "    relevant_chunks = search_knowledge_base(query, model, index, df, top_k=top_k)\n",
    "    \n",
    "    # Generate response\n",
    "    answer = generate_response(query, relevant_chunks)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"relevant_chunks\": relevant_chunks\n",
    "    }\n",
    "\n",
    "# To use this, you'll need to set your OpenAI API key\n",
    "# openai.api_key = \"your-api-key-here\"\n",
    "\n",
    "def test_local_rag_system(queries: List[str]):\n",
    "    for query in queries:\n",
    "        print(f\"\\n\\nQuestion: {query}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get relevant chunks\n",
    "        relevant_chunks = search_knowledge_base(query, model, index, df)\n",
    "        \n",
    "        # Generate response using local model\n",
    "        response = generate_response_hf(query, relevant_chunks)\n",
    "        \n",
    "        print(\"\\nAnswer:\")\n",
    "        print(response)\n",
    "        \n",
    "        print(\"\\nRelevant Documentation Used:\")\n",
    "        for i, chunk in enumerate(relevant_chunks, 1):\n",
    "            print(f\"\\n{i}. {chunk['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9325f9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Question: What is CUDA and how does it work?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe9f29b7734457287dd6e457ec6b880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makaa\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\makaa\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf8a1699aee4332b7535b9414f35137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5934f621334348aba3561081a2da4cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7e97a72cc34bbe96e13dfd7f1cae9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931f8e8c31d14e0fa65cb048260cb420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c974dc9d5142ef9ae544596855e70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a7a951beb54360afed0cbf3ac66f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\makaa\\anaconda3\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "CUDA Runtime The driver API is implemented in the cuda dynamic library cuda dll or cuda so which is copied on the system during the installation of the device driver All its entry points are prefixed with cu It is a handle based imperative API Most objects are referenced by opaque handles that may be spec ified to functions to manipulate the objects The objects available in the driver API are summarized in Table 22 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function Table 24 Object Device Context Module Function\n",
      "\n",
      "Relevant Documentation Used:\n",
      "\n",
      "1. C Programming Guide Release 12 8 498 Chapter 19 Compute Capabilities Chapter 20 Driver API This section assumes knowledge of the concepts described in CUDA Runtime The driver API is implemented in the...\n",
      "\n",
      "2. introduces the low level driver API CUDA Environment Variables lists all the CUDA environment variables Unified Memory Programming introduces the Unified Memory programming model 9 CUDA C Programming ...\n",
      "\n",
      "3. cudaSetDevice will now explicitly initialize the runtime after changing the current device for the host thread Previous versions of CUDA delayed runtime initialization on the new device until the firs...\n",
      "\n",
      "\n",
      "Question: Explain how CUDA threads are organized\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Answer:\n",
      "CUDA programming model\n",
      "\n",
      "Relevant Documentation Used:\n",
      "\n",
      "1. Child Grids have also completed Child A Child thread block or grid is one that has been launched by a Parent grid A Child grid must complete before the Parent Thread Thread Block or Grid are considere...\n",
      "\n",
      "2. introduces the low level driver API CUDA Environment Variables lists all the CUDA environment variables Unified Memory Programming introduces the Unified Memory programming model 9 CUDA C Programming ...\n",
      "\n",
      "3. an example of performing histograms in distributed shared memory 5 3 Memory Hierarchy CUDA threads may access data from multiple memory spaces during their execution as illustrated by Figure 6 Each th...\n",
      "\n",
      "\n",
      "Question: What are the different types of memory in CUDA?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Answer:\n",
      "CUDA Managed Variables variables declared with __managed__ which are semantically similar to a __device__ variable\n",
      "\n",
      "Relevant Documentation Used:\n",
      "\n",
      "1. among others Linux HMM requires Linux kernel version 6 1 24 6 2 11 or 6 3 devices with compute capability 7 5 or higher and a CUDA driver version 535 installed with Open Kernel Modules concurrentManag...\n",
      "\n",
      "2. Allocated Memory 22 1 1 System Requirements for Unified Memory The following table shows the different levels of support for CUDA Unified Memory the device prop erties required to detect these levels ...\n",
      "\n",
      "3. memory copy algorithm such as std memcpy instead 22 3 Unified memory on devices without full CUDA Unified Memory support 22 3 1 Unified memory on devices with only CUDA Managed Memory support For devi...\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What is CUDA and how does it work?\",\n",
    "    \"Explain how CUDA threads are organized\",\n",
    "    \"What are the different types of memory in CUDA?\"\n",
    "]\n",
    "\n",
    "test_local_rag_system(test_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e4871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
